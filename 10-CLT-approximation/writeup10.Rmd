---
title: "Central Limit Theorem Shortcut Deliverable"
author: 'Dong Chen'
date: "12-02"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Introduction

The central limit theorem is an important computational short-cut for generating and making inference from the sampling distribution of the mean. I’ll recall that the central limit theorem short-cut relies on a number of conditions, specifically:

1.  Independent observations
2.  Identically distributed observations
3.  Mean and variance exist
4.  Sample size large enough for convergence

## Background

I am going to compare the sampling distribution of the mean generated by simulation to the sampling distribution implied by the central limit theorem. You will compare the distributions graphically in QQ-plots.
 
# Methods

This will be a 4 × 4 factorial experiment. The first factor will be the sample size, with N = 5, 10, 20, and 40. The second factor will be the degree of skewness in the underlying distribution. The underlying distribution will be the Skew-Normal distribution. The Skew-Normal distribution has three parameters: location, scale, and slant. When the slant parameter is 0, the distribution reverts to the normal distribution. As the slant parameter increases, the distribution becomes increasingly skewed. In this simulation, slant will be set to 0, 2, 10, 100. Set location and scale to 0 and 1, respectively, for all simulation settings.

Use the `rsn` function in the `sn` package.

The output of each combination of factors will be a QQ-plot. Generate a sampling distribution of 5000 draws from both the CLT approximation and the simulation approximation. When analyzing data, the parameters for the mean and variance (in the case of the CLT shortcut) or the parameters for the distribution (in the case of MLE, MM, etc) are replaced with sample estimates. (This is often called the plug-in approach.) For the purposes of this simulation, treat the mean and variance as known values and use the actual population parameters and the population mean and variance instead of sample estimates.

```{r}
library(magrittr)
library(sn)
```

## Set parameters and calculate population mean and sd

```{r}
# Parameters that do not change
R <- 5000; location <- 0; scale <- 1

# Parameters that change
N <- 5
slant <- 10
delta <- slant/(sqrt(1+slant^2))
                  
# Quantites to calculate/generate
pop_mean <- location + scale*delta*(sqrt(2/pi))
pop_sd <- sqrt(scale^2*(1-((2*delta^2)/pi)))
```

In the above code, I set the parameters, divided into constant parameters and changing parameters, and defined the population mean and population standard deviation.

## Calculate sample mean by CLT and Simulation

```{r}
sim_norm <- rnorm(R)
sample_dist_clt <- sim_norm*(pop_sd/sqrt(N))+pop_mean

#Code from asking
random.skew <- array(rsn(R*N,xi=location,omega = scale, alpha=slant), dim = c(R,N))
sample_dist_sim <- apply(random.skew, 1, mean)
```

In the above code, I build a normal random distribution and a skew-random distribution to simulate sample mean by the central limit theorem and by simulation.

## QQPlot

```{r}
# QQ plot
qqplot(sample_dist_sim, sample_dist_clt, asp = 1)
abline(0,1)
```

It can be seen that the fitting is better in the middle range, but the simulation at both ends are smaller. It shows that SIM method is more concentrated. We can test using `quantile` function.

## Dislay the 95% CI from the clt and the sim

```{r}
# Optional (Dislay the 95% CI from the clt and the sim)
quantile(sample_dist_clt,c(0.025,0.975))
quantile(sample_dist_sim,c(0.025,0.975))
```

Method: CLT 95% CI is from `r quantile(sample_dist_clt,c(0.025,0.975))[1]` to `r quantile(sample_dist_clt,c(0.025,0.975))[2]`.

Method: SIM 95% CI is from `r quantile(sample_dist_sim,c(0.025,0.975))[1]` to `r quantile(sample_dist_sim,c(0.025,0.975))[2]`. It shows that SIM method is more concentrated.


## QQPlot in different situation

```{r fig.width=10, fig.height=10}
slant <- c(0,2,10,100)
N <- c(5,10,20,40)

par(mfrow = c(4, 5))
for (i in seq(1, 4)){
  curve(dsn(x, xi = location, omega = scale, alpha = slant[i]), xlim = c(-5, 6), ylim = c(), main = "Distribution", ylab = paste("slant = ", slant[i]), xlab = "")
  for (j in seq(1, 4)){
    delta <- slant[i]/(sqrt(1 + slant[i]^2))
    pop_mean <- location + scale*delta*(sqrt(2/pi))
    pop_sd <- sqrt(scale^2*(1-((2*delta^2)/pi)))
    
    
    sim_norm <- rnorm(R)
    sample_dist_clt <- sim_norm*(pop_sd/sqrt(N[j]))+pop_mean
    
    random.skew <- array(rsn(R*N[j],xi=location,omega = scale, alpha=slant[i]), dim = c(R,N[j]))
    sample_dist_sim <- apply(random.skew, 1, mean)
    
    
    qqplot(sample_dist_sim, sample_dist_clt, asp = 1,  main=paste("N = ", N[j]))
    abline(0, 1)
  }
}
```

From the figure point of view, the overall degree of fit is reasonable. Fixed slant, with the increase of N, the curve change is not obvious, but QQplot is still slightly closer to y=x.Because the more samples, the more stable. In theory, the larger the slant, the greater the difference in distribution, and the curve should be more curved. However, the difference between slant=10 is the most obvious on the picture. May be caused by errors.

# BONUS

```{r fig.width=10, fig.height=10}
par(mfrow = c(4, 5))
for (i in seq(1, 4)){
  curve(dsn(x, xi = location, omega = scale, alpha = slant[i]), xlim = c(-5, 6), ylim = c(), main = "Distribution", ylab = paste("slant = ", slant[i]), xlab = "")
  
  for (j in seq(1, 4)){
    sam <- rsn(N[j], xi = location, omega = scale, alpha = slant[i])
    sample_mean = mean(sam)
    sample_sd <- sqrt(sum((sam- sample_mean)^2/(length(sam)-1)))
    
    
    sim_norm <- rnorm(R)
    sample_dist_clt <- sim_norm*( sample_sd/sqrt(N[j]))+ sample_mean
    
    random.skew <- array(rsn(R*N[j],xi=location,omega = scale, alpha=slant[i]), dim = c(R,N[j]))
    sample_dist_sim <- apply(random.skew, 1, mean)
    
    
    qqplot(sample_dist_sim, sample_dist_clt, asp = 1,  main=paste("N = ", N[j]))
    abline(0, 1)
  }
}
```

When using the sample mean instead of the population mean, I repeated the experiment several times and found that the graphs drawn each time are very different. This is one of the figures. This is because the mean and variance are closely related to the distribution itself. Judging from the experimental results, these are of little reference. But in theory, the larger the number of samples, the more accurate.

